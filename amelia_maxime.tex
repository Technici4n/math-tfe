\section{Les premiere intelligence artificielle}

\subsection{Introduction et définition}

Nous pouvons tout d’abord définir l’intelligence artificielle.
L'intelligence artificielle (IA, ou AI en anglais pour Artificial Intelligence) a pour but de mimer ou d’imiter une certaine forme d’intelligence réelle. Elle essaie de se substituer à la logique de la prise de décision d’un humain en mettant en œuvre différentes techniques.

L'intelligence artificielle est définie comme « l'ensemble de théories et de techniques mises en œuvre en vue de réaliser des machines capables de simuler l'intelligence » . 
Elle correspond donc à un ensemble de concepts et de technologies plus qu'à une discipline autonome constituée.

La notion d’intelligence artificielle est née dans les années 50 et a été mise en avant par deux chercheurs.
Tout d’abord, le mathématicien Alan Turing a débuté ses recherches dans ce domaine car il se demandait si une machine pouvait penser.
Dans son article Computing Machinery and Intelligence , il cherche à savoir s’il est possible d’apporter aux machines une forme d'intelligence. 
Turing a analysé le problème et a proposé une expérience connue aujourd’hui sous le nom de « Test de Turing ». 
Dans ce test, un sujet interagit à l'aveugle avec un autre humain, puis avec une machine programmée pour formuler des réponses sensées. Si le sujet n'est pas capable de faire la différence, alors la machine a réussi le test et, selon l'auteur, peut véritablement être considérée comme « intelligente ».
Ce test permet donc de définir si une machine est consciente.
Ensuite, Warren Weaver a publié en 1949 un mémorandum. Celui-ci se base sur la traduction automatique des langues, ce qui signifie qu’une machine est capable de faire des tâches dite humaines.
Il est principalement connu comme un des pionniers de la traduction automatique

L’intelligence artificielle en tant que  domaine de recherche a été créée lors d’une conférence qui se déroulait sur le campus de Dartmouth College pendant l'été 1956.

Les organisateurs de la conférence de Dartmouth prévoient que soient abordés diverses questions
autour de l'idée de machine pensante :
Comment simuler la pensée et le langage au travers de règles formelles?
Comment faire penser un réseau de neurones?
Comment doter une machine de capacité d'apprentissage automatique?
Comment doter une machine de créativité?
Néanmoins, les discussions seront assez limitées, et la conférence de Darmouth n'aura servi qu'à forger un
embryon de communauté de recherche autour des problématiques évoquées.
McCarthy propose le terme d'intelligence artificielle pour désigner la nouvelle discipline qui va
connaître son âge d'or durant les 15 années qui suivent.


Elle s’est ensuite fortement développée aux États-Unis grâce à certaines personnes de renom dont John McCarthy à l'université Stanford, Marvin Minsky au MIT, Allen Newell et Hebert Simon à l’université de Carnegie-Mellon et pour finir de Donald Michie à l’université d’Edimbourg.
La plupart d’entre eux ont reçu des prix Turing : Marvin Minsky en 1969 ; John McCarthy en 1971; Allen Newell et Herbert Simon en 1975.
D’autres piliers de l’intelligence artificielle ont reçu ces prix : Edward Feigenbaum et Raj Reddy en 1994 ainsi que Judea Pearl en 2011.

Nous détaillerons tout cela plus loin.




\subsection{Quelques grands précurseurs de l'IA}

\subsubsection{ Des scientifiques et des ingénieur}


\textbf{Konrad Zuse} (né le 22/06/1910 – Décédé le 18/12/1995) est un ingénieur allemand et l’un des fondateurs du calcul programmable. Entre  1936 et 1938, Konrad Zuze développe Z1, le premier calculateur mécanique fonctionnant un moteur électrique. En parallèle de la conception de Z1, en 1937, l’ingénieur allemand crée le premier calculateur électro-mécanique programmable binaire à virgule flottante, le Z3. Il est opérationnel en 1941 soit 4 ans de travail.


\textbf{Claude Shannon} (né le 30/04/1916 – Mort le 24/02/2002) est un ingénieur électricien et un mathématicien américain. Il est l’un des pères fondateurs de la « Théorie de l’information ». Il donne son nom à un schéma qui prend le nom de « Schéma de Shannon ». Cet outil est utilisé en Sciences Humaines et en dans  la Communication. Lorsqu’il était mathématicien, il utilise l’Algèbre de Boole dans le but de mettre en place des circuits de commutation. Il apporte sa pierre à l’édifice en mettant en place un outil théorique aux concepteurs de circuits logiques. Il se tourne vers l’informatique. Il commence à créer une machine à jongler, une souris parcourant les labyrinthes, une machine qui résout le cube de Rubik, une autre machine permettant de jouer aux échecs.

 
\textbf{Allen Newell} (né le 19/03/192 - mort en 19/07/1992) a été chercheur en informatique ainsi qu'en psychologie cognitive au sein de deux organismes : RAND Corporation et à la Carnegie-Mellon’s School of Computer Science aux Etats Unis. Il participe à l'élaboration de différents programmes : "Information Processing Language" en 1956 et à deux programmes en Intelligence Artificielle "The Logic Theory Machine" (1956) et le "General Problem Solver" (1957). Pour le dernier programme, il travaille en collaboration avec Herver Simon.


\textbf{John Mc Marthy }(né le 04/09/1927 - Mort le 24/10/2011) est l'un des principaux pionniers de l'Intelligence Artificielle avec Marvin Minsky. Il met l'accent sur la logique symbolique. En 1948 John McCarthy obtient un Bachelor of Science en mathématiques au California Institute of Technology, puis un doctorat à Princeton en 1951. Sa thèse porte sur un certain type d'équations aux dérivées partielles, mais son passage à Princeton lui fait rencontrer Minsky avec qui il se découvre une passion commune pour l'idée de machine pensante. A l'âge de 28ans, il élabore un algorithme permettant d'évaluer et jouant un rôle majeur dans la programmation en Intelligence Arficielle.  Cette algorithme est utilisé dans le plus part des programmes d'échecs. En 1958, il met au point le langage LISP. Il crée à l'Université Standord le laboratoire de l'Intelligence Artificielle. 




\subsubsection{Un économiste}

\textbf{Herbert Simon} (né le 15/06/1916 - mort le 09/02/2001) était avant tout un économiste et un sociologue américain. Il a utilisé pour la première fois les ordinateurs et a conclu que l'ordinateur avait deux rôles à savoir la reproduction de la pensée humaine et la systématisation la pensée humaine. Sa rencontre avec Allen Newell a été primordiale car il s'est penché sur les activités intellectuelles humaines et a découvert qu'elles pouvaient etre automatisées. Ils ont conçu le "General Program Solver" en 1957. Ils ont développé l'Intelligence Artificielle  de différentes manières. 



\subsubsection{Les influences littéraires}


\textbf{Isaac Asimov} (né le 02/01/1920 en Russie - Mort le 06/04/1992 aux Etats Unis) est un écrivain américain. Il a ecrit des oeuvres de science-fiction ainsi que les livres de vulguratisation scientifique. Il a écrit une série d'histoires (série des Robots)  sur les rapports conflictuels entre l'homme et la machine et le role de robots.   


\textbf{Hubert Dreyfus }(né le 15/10/1929) est un professeur américain de philosophie à l'université de Californie. Il s'intéresse à différents sujets comme le phénoménologie, l'existentialisme, la philosophie de la psychologie, la littérature et bien sût l'Intelligence Artificelle. Il critique fortement Allen Newel et Herbert Simon dans le livre "Alchemy and Artificiel Intelligence"

 



\subsection{Apparition des premiers ordinateurs}


Les années 1940 et 1950 voient l'apparition des premiers véritables ordinateurs.
Ils sont Turing complets et électroniques, donc (relativement) rapides. Les entrées et sorties se font par cartes perforées et impression papier. La programmation de telles machines est compliquée et très longue. Il n'existe que très peu d'ordinateurs, uniquement dans quelques universités ou grandes entreprises. Ces machines couteuses servent surtout à faire des calculs massifs(statistiques pour l'état, calculs
scientifique pour la recherchenucléaire, calculs balistiques pourl'armée, ...). Par exemple, l'UNIVAC I (Universel Automatic Computer) est installé en 1951 au bureau du recensement américain.

Notons bien que l'informatique n'existe pas encore. Les spécialistes des ordinateurs sont en effet essentiellement des mathématiciens ou des électroniciens. Les langages de programmation au sens moderne du terme n'existent pas encore : le premier langage évolué, FORTRAN (FORmula TRANslator) ne verra le jour qu'en 1954. 
Neanmois l'apparition des ordinateurs semble rendre possible le rêve de l'IA. Les deux approches de l'IA vont émerger dans les années 1940 a savoir le connexionnisme et le cognitivisme.

Le connexionnisme modélise les phénomènes mentaux ou comportementaux comme des processus émergents de réseaux d'unités simples interconnectées. Le plus souvent les connexionnistes modélisent ces phénomènes à l'aide de réseaux de neurones.

Le cognitivisme, lui, est le courant de recherche scientifique endossant l'hypothèse selon laquelle la pensée est analogue à un processus de traitement de l'information. Il considère que la pensée peut être décrite à un niveau abstrait comme manipulation de symboles, indépendamment du support matériel de cette manipulation (cerveau, machine électronique, etc).
Elle est définie en lien avec l'intelligence artificielle comme une manipulation de symboles ou de représentations symboliques effectuée selon un ensemble de règles. Cette approche établit un lien entre la pensée et le langage (système de symboles).


Le 20eme siecle voit en effet apparaitre plusieurs theories comme la cybernetique et le cognitivisme pour modeliser l'esprit, le cerveau et le mode de fonctionnement de la pensee.

De surcroît, dans le contexte de la guerre froide, la traduction automatique du russe en anglais ou de l'anglais au russe parait cruciale.En 1954, un premier programme, écrit àl'université de Georgetown permet de traduire plusieurs dizaines de phrases simples. Le programme utilise 250 mots et seulement 6 règles de grammaire et tourne sur un IBM 701.

Des crédits sont rapidement alloués aux recherches sur la traduction automatique (aussi bien aux USA qu'en URSS). Les premiers travaux visent la traduction directe, presque mot à mot, à l'aide de dictionnaires bilingues et de règles simples. Les problèmes de polysémie (amateur, blanc)ou d'homonymie (mousse, avocat, …)apparaissent très rapidement.                      




\subsubsection{Les travaux de Simon et Newell}


Allen Newell (1927-1992), après un Bachelor of Science en physique à Stanford, rejoint Princeton en 1949 pour mener une thèse en mathématiques. Durant ses études, il a été fortement influencé par le mathématicien hongrois Georges Polya (1887-1985), qui avait introduit la notion d'heuristique pour la résolution de problème. Une heuristique (du grec eurisko, je trouve) est une méthode empirique de résolution de problème, dont la validité ou l'efficacité n'est pas prouvée. Par exemple, protéger la reine aux échecs, choisir la caisse où la file est la plus courte, ...
Trouvant finalement les mathématiques trop abstraites, Newell accepte en 1950 un poste à la RAND Corporation de Santa Monica, pour mener des travaux plus concrets, sur l'aéronautique de défense notamment.
Simon est également consultant à la RAND (Research ANd Development). La RAND, créée pour étudier la mise au point d'un satellite artificiel, va peu à peu étendre ses travaux à l'informatique, l'économie, la géopolitique, etc. Les idées de Simon et de Newell convergent : La rationalité limitée de Simon implique que la prise de décision repose sur des procédures permettant de palier aux manques d'information en tenant compte du contexte. Pour Newell, ces procédures sont des heuristiques.

Simon et Newell considèrent que la condition nécessaire et suffisante pour qu'une machine puisse faire preuve d'intelligence est qu'elle soit un système physique de symboles.

Mais ils mettent au coeur de leurs travaux la notion d'heuristique : être intelligent, c'est aussi être capable de construire des heuristiques, de les tester, de les faire évoluer. Aidés par un programmeur de la RAND, Cliff Shaw, ils développent Logic Theorist en 1956, un programme de démonstration automatique de théorème (voir l'article The logic theory machine: A Complex Information Processing System, 1956). Logic Theorist est considéré comme le premier programme informatique relevant du
domaine de l'IA.

Newell et Simon y demontrent une serie de théorèmes et envoient un article avec cette nouvelle démonstration au Journal of Symbolic Logic, en ajoutant Logic Theorist comme co-auteur. Mais l'article est refusé au motif que ce théorème est déjà démontré depuis longtemps. Simon écrit dans son autobiographie : « nous avons inventé un programme informatique capable de penser de façon non-numérique et, de ce fait, avons résolu le vénérable problème de l'âme et du corps, en expliquant comment un système composé de matière pouvait exhiber les propriétés de l'esprit » (Models of My Life, H.
Simon, 1991).

Les travaux de Newell et Simon illustrent également les apports croisés entre l'informatique et l'intelligence artificielle. On a d'une part le développement de l'informatique rend possible des expériences en IA. Et d'autre part les problèmes posées par les expériences en IA conduisent à produire des outils qui servent le développement de l'informatique.
Pour faciliter la programmation du Logic Theorist, Newel, Simon et Shaw développent le
langage IPL (Information Processing Language) en 1956.










\subsection{Le Japon et l'IA}


Bénéficiant des techniques de l'intelligence artificielle, la transmission des savoirs vit au début des années 90 une révolution. Les concepts sont bouleversés. Ils s'enrichissent de l'apport du couple IA-Robotique qui crée ses propres spécificités.

Si les robots font déjà partie intégrante du paysage industriel japonais, où ils servent à réaliser des tâches dangereuses ou nécessitant une haute précision de manière automatique, une évolution majeure du marché se dessine avec l’émergence de la robotique de service et, dans une moindre mesure, de la robotique de loisir.

Les Japonais sont depuis très longtemps les champions de la robotique et on ne compte plus leurs inventions. Les recherches actuelles concernent tout particulièrement les robots « humanoïdes », qui tentent d’imiter les humains et leurs capacités.


Par exemple les robots humanoïdes présentent plus de difficultés de développement, puisqu’il s’agit d’imiter des facultés humaines, ce qui n’est pas simple. Le Japon rencontre un problème très particulier avec une natalité très faible et donc un vieillissement très important de la population qui nécessite de plus en plus d’aide. C’est pour cette raison que Honda continue de travailler sur ASIMO, le robot humanoïde qui devrait aider les personnes âgées à rester à leur domicile, en les assistant dans de nombreuses tâches qu’ils ne peuvent assumer. Par ailleurs, des chercheurs de l’université de Tohoku, associés à des chercheurs français, consacrent leurs travaux à la résolution d’un problème très important : permettre à un robot humanoïde, HRP-2, de se déplacer sur des surfaces irrégulières. En effet, ce type de robot ne peut se déplacer que sur une surface dure et stable, ce qui limite fortement son champs de manœuvre. La résolution de ce problème représenterait une avancée considérable en robotique et donc egalement en intelligence artificielle.




\subsection{Aujourd'hui}

D’années en années, l'IA a été implémentée dans de plus en plus de domaines d'application et le développement de celle-ci est au centre de beaucoup de recherches actuelles. 
Le développement des technologies informatiques et des techniques algorithmiques ont eu une telle croissance ces dernières années que les machines dépassent l’homme dans plusieurs domaine comme le jeu d’échecs ou encore le jeu de go.

Toutefois, les sujets concernés par l’IA ont varier au cours du temps. Par exemple, dans les années 1950, la recherche d'un itinéraire était considéré comme un problème d'intelligence artificielle, alors qu’aujourd’hui, il existe différentes applications qui sur base d’algorithmes résolvent ces questions de recherche d'itinéraires et il ne s’agit plus d’intelligence artificielle.

Aujourd’hui toutes les grandes entreprises dans le monde de l’informatique (Google, Microsoft, Apple, IBM ou Facebook) portent une attention toute particulière aux problématiques de l'intelligence artificielle en tentant de l'appliquer à différents domaines précis. 
Chacun met ainsi en place des réseaux de neurones artificiels constitués de serveurs afin de traiter de lourds calculs au sein de gigantesques bases de données.

Aujourd’hui, l’intelligence artificielle est de plus en plus utilisée pour être au service des humains. Ainsi, par exemple, la vision artificielle comme nous l’expliquerons dans ce travail, permet à la machine de déterminer précisément le contenu d'une image comme des pièces de monnaies pour ensuite la classer automatiquement selon l'objet, la couleur ou le visage repéré. 

La reconnaissance vocale a également le vent en poupe avec des assistants virtuels capables de transcrire les propos formulés en langage naturel puis de traiter les requêtes soit en répondant directement via une synthèse vocale, soit avec une traduction instantanée ou encore en effectuant une requête relative à la commande. Apple était en la matière un des précurseurs avec son application « SIRI »

Nous pouvons donc affirmer que l’IA qui était au départ très restreinte a un potentiel infini. 
Sans cesse, de jour en jour, les recherches progressent et les résultats de ce que l’intelligence artificielle est capable de réaliser croît de manière exponentielle.




